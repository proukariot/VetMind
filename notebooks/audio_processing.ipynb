{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfde111",
   "metadata": {},
   "source": [
    "# Audio Processing\n",
    "\n",
    "In this notebook several STT models are tested on the test dataset, generated in the Audio Generation notebook (`audio_generation.ipynb`).\n",
    "\n",
    "For transcription let's have to steps:\n",
    "1. Define the language\n",
    "2. Use the corresponding Vosk model for transcription\n",
    "\n",
    "Metric: `WER` metric is used. This metric has values from 0 (for the same text) to infinity. For (en, pl, ru) languages corresponding Vosk models have `WER` in [0.18, 0.20]. These values could be used as baselines for the future improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfa21d",
   "metadata": {},
   "source": [
    "## 1. Configure\n",
    "\n",
    "* `faster-whisper` model is used for the language detection\n",
    "* `vosk` model is used for the language trascription\n",
    "* `jiwer` library contains WER metric, which is used for the language trascription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faster-whisper -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vosk -q\n",
    "!pip install soundfile -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89c327",
   "metadata": {},
   "source": [
    "### 1.1 Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSET_PATH = \"../tests/audio_data/audio_testing_data.json\"\n",
    "AUDIO_ROOT = \"../tests/audio_data/audio/\"\n",
    "\n",
    "# how many seconds of audio is used for the lang detection\n",
    "HEAD_DURATION = 5.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a03a590",
   "metadata": {},
   "source": [
    "## 2. Language Detection\n",
    "\n",
    "For Language Detection `whisper` model is used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8b078",
   "metadata": {},
   "source": [
    "### 2.1 Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\"tiny\", device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One test file\n",
    "test_file = wav_path = f\"{AUDIO_ROOT}en_003.wav\"\n",
    "segments, info = model.transcribe(\n",
    "    test_file,\n",
    "    language=None,\n",
    "    clip_timestamps=[0.0, 1.0],\n",
    "    vad_filter=True,\n",
    ")\n",
    "print(info.language, info.language_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8cb83",
   "metadata": {},
   "source": [
    "### 2.2 Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the test data\n",
    "with open(TESTSET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_set = json.load(f)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# Main loop\n",
    "correct_count = 0\n",
    "for entry in tqdm.tqdm(test_set):\n",
    "    expected_lang = entry[\"language\"]\n",
    "    wav_path = f\"{AUDIO_ROOT}{entry[\"id\"]}.wav\"\n",
    "    segments, info = model.transcribe(\n",
    "        wav_path,\n",
    "        language=None,\n",
    "        clip_timestamps=[0.0, HEAD_DURATION],  # first HEAD_DURATION seconds\n",
    "        vad_filter=True,\n",
    "    )\n",
    "    correct_count += (info.language == expected_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_count = len(test_set)\n",
    "lang_detection_accuracy = correct_count / total_count\n",
    "lang_detection_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7626c03",
   "metadata": {},
   "source": [
    "All the test data language detected correctly, so we can use `whisper` model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa264751",
   "metadata": {},
   "source": [
    "## 3. Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ff614",
   "metadata": {},
   "source": [
    "### 3.1 Metrics for the trascription\n",
    "\n",
    "`WER` metric is used. This metric has values from 0 (for the same text) to infinity.\n",
    "\n",
    "Vosk API allow to add custom words: `rec = KaldiRecognizer(model, samplerate, json.dumps(custom_words))`. But such run didn't return correct result (just an empty string is returned).\n",
    "\n",
    "That's why we calculate WER of the normalized texts (without punctuation and in the lower case). \n",
    "But this problem should be fixed in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer  # WER metric\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import re\n",
    "\n",
    "# read the audio\n",
    "def read_audio(file_path): \n",
    "    data, sample_rate = sf.read(file_path)\n",
    "    data = np.int16(data * 32767)\n",
    "    data = data.tobytes()\n",
    "    return data\n",
    "\n",
    "\n",
    "# transcript audio\n",
    "def transcript_audio(file_path, recognizer):\n",
    "    audio_data = read_audio(file_path)\n",
    "    recognizer.AcceptWaveform(audio_data)\n",
    "    result = recognizer.Result()\n",
    "    res_text = json.loads(result).get(\"text\", \"\")\n",
    "    return res_text\n",
    "\n",
    "\n",
    "# text normalization for WER\n",
    "def normalize_for_wer(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text) \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# calculate wer for several texts (not just a mean)\n",
    "def calculate_wer(actual_texts, expected_texts):\n",
    "    # normalization\n",
    "    actual_texts = [normalize_for_wer(actual) for actual in actual_texts]\n",
    "    expected_texts = [normalize_for_wer(expected) for expected in expected_texts]\n",
    "    \n",
    "    wer_value = jiwer.wer(actual_texts, expected_texts)\n",
    "\n",
    "    return wer_value\n",
    "\n",
    "def get_wer(test_df, language, rec, is_debug=False):\n",
    "    test_df_lang = [item for item in test_df if item[\"language\"] == language]\n",
    "    actual_texts = [transcript_audio(f\"{AUDIO_ROOT}{item[\"id\"]}.wav\", rec) for item in test_df_lang]\n",
    "    expected_texts = [item[\"text\"] for item in test_df_lang]\n",
    "    if is_debug:\n",
    "        print(actual_texts[0], expected_texts[0])\n",
    "    wer = calculate_wer(actual_texts, expected_texts)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "DATASET_PATH = \"../tests/audio_data/audio_testing_data.json\"\n",
    "AUDIO_ROOT = \"../tests/audio_data/audio/\"\n",
    "\n",
    "# Load the test data\n",
    "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_df = json.load(f)[\"data\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case \n",
    "test_id = \"en_001\"\n",
    "test_data = next(item for item in test_df if item[\"id\"] == test_id)\n",
    "expected_text = test_data[\"text\"]\n",
    "\n",
    "print(expected_text)\n",
    "actual_text = \"you hello what seems to be the problem our dog is limping all right let me take a look have you given any medication just some iodine i see i recommend car profaned and a cooling gel keep the dog come thankyou goodbye\"\n",
    "\n",
    "wer = jiwer.wer(actual_text, expected_text)\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff049edf",
   "metadata": {},
   "source": [
    "### 3.2 Vosk English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247553ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "# Disable Vosk logs\n",
    "SetLogLevel(-1)\n",
    "\n",
    "MODEL_DIR = \"../models/audio_processing/vosk-model-small-en-us-0.15\"\n",
    "\n",
    "sample_rate = 16000\n",
    "language = \"en\"\n",
    "\n",
    "# 1. Model loading\n",
    "model = Model(MODEL_DIR)\n",
    "\n",
    "# 2. Recognizer\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "# 3. Calculate WER on English data set\n",
    "wer = get_wer(test_df, language, rec)\n",
    "print(f\"WER for {language} is {wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c78cf",
   "metadata": {},
   "source": [
    "WER value 0.19 could be a baseline for English language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e8bab",
   "metadata": {},
   "source": [
    "### 3.3 Vosk Polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aefc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "# Disable Vosk logs\n",
    "SetLogLevel(-1)\n",
    "\n",
    "MODEL_DIR = \"../models/audio_processing/vosk-model-small-pl-0.22\"\n",
    "\n",
    "sample_rate = 16000\n",
    "language = \"pl\"\n",
    "\n",
    "# 1. Model loading\n",
    "model = Model(MODEL_DIR)\n",
    "\n",
    "# 2. Recognizer\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "# 3. Calculate WER on Polish data set\n",
    "wer = get_wer(test_df, language, rec)\n",
    "print(f\"WER for {language} is {wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e1699",
   "metadata": {},
   "source": [
    "### 3.4 Vosk Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac13727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "# Disable Vosk logs\n",
    "SetLogLevel(-1)\n",
    "\n",
    "MODEL_DIR = \"../models/audio_processing/vosk-model-small-ru-0.22\"\n",
    "\n",
    "sample_rate = 16000\n",
    "language = \"ru\"\n",
    "\n",
    "# 1. Model loading\n",
    "model = Model(MODEL_DIR)\n",
    "\n",
    "# 2. Recognizer\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "# 3. Calculate WER on Russian data set\n",
    "wer = get_wer(test_df, language, rec, is_debug=False)\n",
    "print(f\"WER for {language} is {wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dae844",
   "metadata": {},
   "source": [
    "## 4. Apply in real recording "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_DIR = \"../models/audio_processing/vosk-model-small-en-us-0.15\"\n",
    "\n",
    "recording_path = \"../data/Recordings/recording_21_20251230_160503.wav\"\n",
    "\n",
    "sample_rate = 16000\n",
    "language = \"en\"\n",
    "\n",
    "# 1. Model loading\n",
    "model = Model(MODEL_DIR)\n",
    "\n",
    "# 2. Recognizer\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "text = transcript_audio(recording_path, rec)\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
